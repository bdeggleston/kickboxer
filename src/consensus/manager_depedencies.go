package consensus

import (
	"fmt"
	"sync"
)

/**
the dependency map maintains a nested hierarchy of pointers to instances
that depend on each other.

here's why the dependency map works the way it does:
let's assume that we need to serialize operations around a nested hash table,
given the following operations:
store[a][b][c] = 5
store[a][b][d] = 6
although the operations operate on the same root key (a), they do not interfere
with each other, therefore, they don't need to be serialized. However, they
would interfere with these operations:
store[a][b] = {} // reset to empty hash map
delete store[a]

When a new instance is received, an array of "interfering" keys is generated by
the store for the instance, which describe the hierarchy of keys that this
instruction depends on, from general to specific. For the query `store[a][b][c] = 5`,
the array of interfering keys would be [a b c]. The instance would be dependent
on all operations on [a], [a][b], and [a][b][c], as well as any queries on children
of 'c', although the child keys will not be included in the interfering keys.

Determining dependencies in this way avoids having to check every recent instance
for interference, and also prevents dependency graph splits caused by not checking
executed instances, with instances being executed at differnt times on different nodes.

For a given tuple of keys, only the most recent write is kept. Additionally, read queries
do not gain dependencies on each other, only writes. Writes, however, gain dependencies
on reads.

Here are the use cases, and the expected behavior:
	If this is the first time a root key is seen, a dependencies object should be created, but there should be
		no dependencies
	If this is the first time a sub key is seen, dependencies objects should be created.
	If a node is written to, all child nodes should be taken as dependencies, and removed
	If a node is read from, child writes should be taken as dependencies (should they be removed?)

Areas for optimization:
  for read heavy workloads, we don't want reads to pile up indefinitely, so there needs
  to be a method of removing them after they've been executed.
	  Periodic 'checkpoint' queries would work, but we need to solve the problem of serializing
		  queries across the entire cluster, and not just keys.
	  Another potential solution would be for read queries to be periodically taken as dependencies
		  for reads, and then removed.

 */

type dependencyMap struct {
	deps map[string]*dependencies
	lock sync.RWMutex
}

func newDependencyMap () *dependencyMap {
	return &dependencyMap{deps: make(map[string]*dependencies)}
}

func (dm *dependencyMap) get(key string) *dependencies {
	dm.lock.RLock()
	deps := dm.deps[key]
	dm.lock.RUnlock()
	if deps != nil {
		return deps
	}

	dm.lock.Lock()
	deps = dm.deps[key]
	if deps == nil {
		deps = newDependencies()
		dm.deps[key] = deps
	}
	dm.lock.Unlock()
	return deps
}

func (dm *dependencyMap) all() []*dependencies {
	dm.lock.RLock()
	defer dm.lock.RUnlock()

	if len(dm.deps) == 0 {
		return []*dependencies{}
	}

	deps := make([]*dependencies, len(dm.deps))
	idx := 0
	for _, dep := range dm.deps {
		deps[idx] = dep
		idx++
	}

	return deps
}

type dependencies struct {
	lastWrite *Instance
	lastReads []*Instance
	lock sync.RWMutex
	subDependencies *dependencyMap
}

func newDependencies() *dependencies {
	return &dependencies{
		lastReads: make([]*Instance, 0),
		subDependencies: newDependencyMap(),
	}
}

func (d *dependencies) getLocalDeps(instance *Instance) []InstanceID {

	var lenDeps int

	if instance.ReadOnly && d.lastWrite != nil {
		lenDeps++
	}

	if !instance.ReadOnly {
		lenDeps += len(d.lastReads)
	}

	deps := make([]InstanceID, lenDeps)

	if !instance.ReadOnly {
		for i := range d.lastReads {
			deps[i] = d.lastReads[i].InstanceID
		}
	}

	if d.lastWrite != nil {
		deps[lenDeps - 1] = d.lastWrite.InstanceID
	}

	return deps
}

func (d *dependencies) getChildDeps(instance *Instance) []InstanceID {
	// setup locks
	if !instance.ReadOnly {
		d.lock.Lock()
		defer d.lock.Unlock()
	} else {
		d.lock.RLock()
		defer d.lock.RUnlock()
	}

	deps := d.getLocalDeps(instance)

	if len(d.subDependencies) > 0 {
		for _, subDeps := range d.subDependencies.all() {
			deps = append(deps, subDeps.getChildDeps()...)
		}
	}

	return deps
}

func (d *dependencies) GetAndSetDeps(keys []string, instance *Instance) []InstanceID {
	// this is the final key, so last writes should be swapped out
	var nextKeys []string
	lastKey := len(keys) == 1

	// setup locks
	if lastKey {
		d.lock.Lock()
		defer d.lock.Unlock()
		nextKeys = []string{}
	} else {
		d.lock.RLock()
		defer d.lock.RUnlock()
		nextKeys = keys[1:]
	}

	deps := d.getLocalDeps(instance)

	if lastKey {
		// get child deps and update reads / writes
		if len(d.subDependencies) > 0 {
			for _, subDeps := range d.subDependencies.all() {
				deps = append(deps, subDeps.getChildDeps()...)
			}
		}
		if instance.ReadOnly {
			d.lastReads = append(d.lastReads, instance)
		} else {
			d.lastWrite = instance
			d.lastReads = make([]*Instance, 0)
			d.subDependencies = newDependencies()
		}

	} else {
		// get deps from the next node in the tree
		subDeps := d.subDependencies.get(nextKeys[0])
		deps = append(deps, subDeps.GetAndSetDeps(nextKeys, instance)...)
	}

	return deps
}

// the root of the dependency tree
type dependencyManager struct {
	depMap map[string]*dependencies
	manager *Manager
	lock sync.RWMutex
}

func (dm *dependencyManager) GetAndSetDeps(instance *Instance) ([]InstanceID, error) {
	keys := dm.manager.cluster.InterferingKeys(instance.Command)

	if len(keys) < 1 {
		return nil, fmt.Errorf("at least one interfering key required, none found")
	}

	dm.lock.RLock()
	deps := dm.depMap[keys[0]]
	dm.lock.RUnlock()

	if deps == nil {
		dm.lock.Lock()
		if deps = dm.depMap[keys[0]]; deps == nil {
			deps = newDependencies()
			dm.depMap[keys[0]] = deps
		}
		dm.lock.Unlock()
	}

	return deps.GetAndSetDeps(keys, instance), nil
}

