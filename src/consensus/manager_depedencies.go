package consensus

import (
	"fmt"
	"sync"
)

/**
the dependency map maintains a nested hierarchy of pointers to instances
that depend on each other.

here's why the dependency map works the way it does:
let's assume that we need to serialize operations around a nested hash table,
given the following operations:
store[a][b][c] = 5
store[a][b][d] = 6
although the operations operate on the same root key (a), they do not interfere
with each other, therefore, they don't need to be serialized. However, they
would interfere with these operations:
store[a][b] = {} // reset to empty hash map
delete store[a]

When a new instance is received, an array of "interfering" keys is generated by
the store for the instance, which describe the hierarchy of keys that this
instruction depends on, from general to specific. For the query `store[a][b][c] = 5`,
the array of interfering keys would be [a b c]. The instance would be dependent
on all operations on [a], [a][b], and [a][b][c], as well as any queries on children
of 'c', although the child keys will not be included in the interfering keys.

Determining dependencies in this way avoids having to check every recent instance
for interference, and also prevents dependency graph splits caused by not checking
executed instances, with instances being executed at differnt times on different nodes.

For a given tuple of keys, only the most recent write is kept. Additionally, read queries
do not gain dependencies on each other, only writes. Writes, however, gain dependencies
on reads.

Here are the use cases, and the expected behavior:
	If this is the first time a root key is seen, a dependencies object should be created, but there should be
		no dependencies
	If this is the first time a sub key is seen, dependencies objects should be created.
	If a node is written to, all child nodes should be taken as dependencies, and removed
	If a node is read from, child writes should be taken as dependencies (should they be removed?)

Areas for optimization:
  for read heavy workloads, we don't want reads to pile up indefinitely, so there needs
  to be a method of removing them after they've been executed.
	  Periodic 'checkpoint' queries would work, but we need to solve the problem of serializing
		  queries across the entire cluster, and not just keys.
	  Another potential solution would be for read queries to be periodically taken as dependencies
		  for reads, and then removed.

 */

type dependencyMap struct {
	deps map[string]*dependencies
	lock sync.RWMutex
}

func newDependencyMap () *dependencyMap {
	return &dependencyMap{deps: make(map[string]*dependencies)}
}

func (dm *dependencyMap) get(key string) *dependencies {
	dm.lock.RLock()
	deps := dm.deps[key]
	dm.lock.RUnlock()
	if deps != nil {
		return deps
	}

	dm.lock.Lock()
	deps = dm.deps[key]
	if deps == nil {
		deps = newDependencies()
		dm.deps[key] = deps
	}
	dm.lock.Unlock()
	return deps
}

func (dm *dependencyMap) all() []*dependencies {
	dm.lock.RLock()
	defer dm.lock.RUnlock()

	if len(dm.deps) == 0 {
		return []*dependencies{}
	}

	deps := make([]*dependencies, len(dm.deps))
	idx := 0
	for _, dep := range dm.deps {
		deps[idx] = dep
		idx++
	}

	return deps
}

type dependencies struct {
	writes InstanceIDSet
	reads InstanceIDSet
	executed InstanceIDSet
	acknowledged InstanceIDSet
	lock sync.RWMutex
	subDependencies *dependencyMap
}

func newDependencies() *dependencies {
	return &dependencies{
		writes: NewSizedInstanceIDSet(0),
		reads: NewSizedInstanceIDSet(0),
		executed: NewSizedInstanceIDSet(0),
		acknowledged: NewSizedInstanceIDSet(0),
		subDependencies: newDependencyMap(),
	}
}

func (d *dependencies) getLocalDeps(instance *Instance) InstanceIDSet {
	deps := d.writes.Copy()
	if !instance.ReadOnly {
		deps.Combine(d.reads)
	}

	return deps
}

func (d *dependencies) getChildDeps(instance *Instance) InstanceIDSet {
	// setup locks
	if !instance.ReadOnly {
		d.lock.Lock()
		defer d.lock.Unlock()
	} else {
		d.lock.RLock()
		defer d.lock.RUnlock()
	}

	deps := d.getLocalDeps(instance)


	if subDependencies := d.subDependencies.all(); len(subDependencies) > 0 {
		for _, subDeps := range subDependencies {
			deps.Combine(subDeps.getChildDeps(instance))
		}
	}

	return deps
}

func (d *dependencies) GetAndSetDeps(keys []string, instance *Instance) InstanceIDSet {
	var nextKeys []string
	lastKey := len(keys) == 1

	// setup locks
	if lastKey {
		d.lock.Lock()
		defer d.lock.Unlock()
		nextKeys = []string{}
	} else {
		d.lock.RLock()
		defer d.lock.RUnlock()
		nextKeys = keys[1:]
	}

	deps := d.getLocalDeps(instance)


	if lastKey {
		// get child deps and update reads / writes
		if subDependencies := d.subDependencies.all(); len(subDependencies) > 0 {
			for _, subDeps := range subDependencies {
				deps.Combine(subDeps.getChildDeps(instance))
			}
		}

		if instance.ReadOnly {
			d.reads.Add(instance.InstanceID)
		} else {
			d.writes.Add(instance.InstanceID)
		}

		// remove executed and acknowledged deps
		exAcked := d.executed.Union(d.acknowledged)
		d.reads.Subtract(exAcked)
		d.writes.Subtract(exAcked)

	} else {
		// get deps from the next node in the tree
		subDeps := d.subDependencies.get(nextKeys[0])
		deps.Combine(subDeps.GetAndSetDeps(nextKeys, instance))
	}

	return deps
}

// when an instance is accepted or committed, it's dependencies can be
// considered acknowledged by the cluster. The instance itself is not,
// because we want to make sure that it's a dependency for at least one
// other instance before it's removed from the dependency manager
func (d *dependencies) ReportAcknowledged(keys []string, instance *Instance) {
	var nextKeys []string
	lastKey := len(keys) == 1

	// setup locks
	if lastKey {
		d.lock.Lock()
		defer d.lock.Unlock()
		nextKeys = []string{}
	} else {
		d.lock.RLock()
		defer d.lock.RUnlock()
		nextKeys = keys[1:]
	}

	if lastKey {
		// we want an instance id to be acknowledged as
		// a dependency before removing it
		// d.acknowledged.Add(instance.InstanceID)
		d.acknowledged.Add(instance.Dependencies...)
	} else {
		subDeps := d.subDependencies.get(nextKeys[0])
		subDeps.ReportAcknowledged(nextKeys, instance)
	}
}

// reports and instance as executed. Once an instance has been acknowledged
// as a dependency and executed, it can be removed from the dependency tree
func (d *dependencies) ReportExecuted(keys []string, instance *Instance) {
	var nextKeys []string
	lastKey := len(keys) == 1

	// setup locks
	if lastKey {
		d.lock.Lock()
		defer d.lock.Unlock()
		nextKeys = []string{}
	} else {
		d.lock.RLock()
		defer d.lock.RUnlock()
		nextKeys = keys[1:]
	}

	if lastKey {
		d.executed.Add(instance.InstanceID)
	} else {
		subDeps := d.subDependencies.get(nextKeys[0])
		subDeps.ReportAcknowledged(nextKeys, instance)
	}
}

// the root of the dependency tree
type dependencyManager struct {
	deps *dependencyMap
	manager *Manager
	lock sync.RWMutex
}

func (dm *dependencyManager) GetAndSetDeps(instance *Instance) ([]InstanceID, error) {
	keys := dm.manager.cluster.InterferingKeys(instance.Command)

	if len(keys) < 1 {
		return nil, fmt.Errorf("at least one interfering key required, none found")
	}

	deps := dm.deps.get(keys[0])
	return deps.GetAndSetDeps(keys, instance).List(), nil
}

// When an instance is committed, it has been acknowledged by a quorum of replicas,
// and it's dependencies can be removed from the dependency manager
func (dm *dependencyManager) ReportAcknowledged(instance *Instance) error {
	keys := dm.manager.cluster.InterferingKeys(instance.Command)

	if len(keys) < 1 {
		return fmt.Errorf("at least one interfering key required, none found")
	}

	deps := dm.deps.get(keys[0])
	deps.ReportAcknowledged(keys, instance)
	return nil
}

func (dm *dependencyManager) ReportExecuted(instance *Instance) error {
	keys := dm.manager.cluster.InterferingKeys(instance.Command)

	if len(keys) < 1 {
		return fmt.Errorf("at least one interfering key required, none found")
	}

	deps := dm.deps.get(keys[0])
	deps.ReportExecuted(keys, instance)
	return nil
}
func newDependencyManager(manager *Manager) *dependencyManager {
	return &dependencyManager{deps: newDependencyMap(), manager: manager}
}

